import copy
import logging
import multiprocessing
import numpy as np
import os
import time

from gunpowder.caffe.net_io_wrapper import NetIoWrapper
from gunpowder.ext import caffe
from gunpowder.nodes.batch_filter import BatchFilter
from gunpowder.producer_pool import ProducerPool, WorkersDied
from gunpowder.roi import Roi, Coordinate
from gunpowder.volume import VolumeTypes, Volume


logger = logging.getLogger(__name__)

class PredictProcessDied(Exception):
    pass

class Predict(BatchFilter):
    '''Augments a batch with network predictions.

    Args:

        prototxt (string): Filename of the network prototxt.

        weights (string): Filename of the network weights.

        inputs (dict): Dictionary from :class:``VolumeType`` to the names of 
            input layers in the network.

        outputs (dict): Dictionary from :class:``VolumeType`` to the names of 
            output layers in the network. New volumes will be generated by this 
            node for each entry (if requested downstream). Set the resolution of 
            the new volume via parameter ``output_resolutions``.

        output_resolutions (dict): Dictionary from :class:``VolumeType`` to 
            :class:``Coordinate``. This sets the resolutions of volumes created 
            by this node.

        use_gpu (int): Which GPU to use. Set to ``None`` for CPU mode.
    '''

    def __init__(self, prototxt, weights, inputs, outputs, output_resolutions, use_gpu=None):

        for f in [prototxt, weights]:
            if not os.path.isfile(f):
                raise RuntimeError("%s does not exist"%f)

        # start prediction as a producer pool, so that we can gracefully exit if
        # anything goes wrong
        self.worker = ProducerPool([lambda gpu=use_gpu: self.__predict(gpu)], queue_size=1)
        self.batch_in = multiprocessing.Queue(maxsize=1)

        self.prototxt = prototxt
        self.weights = weights
        self.net_initialized = False
        self.inputs = inputs
        self.outputs = outputs
        self.output_resolutions = output_resolutions

    def setup(self):

        self.upstream_spec = self.get_upstream_provider().get_spec()
        self.spec = copy.deepcopy(self.upstream_spec)

        for volume_type in self.outputs.keys():
            self.spec.volumes[volume_type] = self.spec.volumes[VolumeTypes.RAW]

        self.worker.start()

    def get_spec(self):
        return self.spec

    def teardown(self):
        self.worker.stop()

    def prepare(self, request):

        # remove request parts that we provide
        for volume_type in self.outputs.keys():
            if volume_type in request.volumes:
                del request.volumes[volume_type]

    def process(self, batch, request):

        self.batch_in.put(batch)

        try:
            out = self.worker.get()
        except WorkersDied:
            raise PredictProcessDied()

        for volume_type in self.outputs.keys():
            batch.volumes[volume_type]     = out.volumes[volume_type]
            batch.volumes[volume_type].roi = request.volumes[volume_type]


    def __predict(self, use_gpu):

        if not self.net_initialized:

            logger.info("Initializing solver...")

            if use_gpu is not None:

                logger.debug("Predict process: using GPU %d"%use_gpu)
                caffe.enumerate_devices(False)
                caffe.set_devices((use_gpu,))
                caffe.set_mode_gpu()
                caffe.select_device(use_gpu, False)

            self.net = caffe.Net(self.prototxt, self.weights, caffe.TEST)
            self.net_io = NetIoWrapper(self.net, self.outputs.values())
            self.net_initialized = True

        start = time.time()

        batch = self.batch_in.get()

        fetch_time = time.time() - start

        self.net_io.set_inputs({
            input_name: batch.volumes[volume_type].data
            for volume_type, input_name in self.inputs.items()
        })

        self.net.forward()
        output = self.net_io.get_outputs()

        predict_time = time.time() - start

        logger.info("Predict process: time=%f (including %f waiting for batch)" % (predict_time, fetch_time))

        for volume_type, output_name in self.outputs.items():
            voxel_size = self.output_resolutions[volume_type]
            batch.volumes[volume_type] = Volume(
                    data=output[output_name][0], # strip #batch dimension
                    roi=Roi(shape=Coordinate([dim for dim in
                                              output[output_name][0].shape[-len(voxel_size):]])*voxel_size)
                # dummy roi, will be corrected in process()
            )

        return batch
