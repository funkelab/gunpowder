import ctypes
import logging
import multiprocessing as mp
import numpy as np

from functools import reduce
from gunpowder.array import ArrayKey, Array
from gunpowder.nodes.generic_predict import GenericPredict
from operator import mul

logger = logging.getLogger(__name__)


class BufferedPredict(GenericPredict):
    '''Buffered implementation of :class:`gunpowder.nodes.Predict`.

    The class runs a background predict process with shared memory buffer
    between the workers to increase performance without having to instantiate
    multiple copies of the network on the GPU.

    Specific frameworks (e.g., Tensorflow, Torch, JAX) should specialize this
    class to make a complete Predict node.

    Args:

        inputs (``dict``, ``string`` -> :class:`ArrayKey`):

            Dictionary from the names of input tensors in the network to
            array keys.

        outputs (``dict``, ``string`` -> :class:`ArrayKey`):

            Dictionary from the names of output tensors in the network to array
            keys. New arrays will be generated by this node for each entry (if
            requested downstream).

        array_specs (``dict``, :class:`ArrayKey` -> :class:`ArraySpec`, optional):

            Used to set the specs of generated arrays (``outputs``). This is
            useful to set the ``voxel_size``, for example, if they differ from
            the voxel size of the input arrays. Only fields that are not
            ``None`` in the given :class:`ArraySpec` will be used.

        skip_empty (``bool``, optional):

            Skip prediction, if all inputs are empty (contain only 0). In this
            case, outputs are simply set to 0.

        max_shared_memory (``int``, optional):

            The maximal amount of shared memory in bytes to allocate to send
            batches to the GPU processes. Defaults to 1GB.
    '''

    def __init__(
            self,
            inputs,
            outputs,
            array_specs=None,
            skip_empty=False,
            max_shared_memory=1024*1024*1024):

        super().__init__(
            inputs,
            outputs,
            array_specs)

        self.skip_empty = skip_empty

        self.manager = mp.Manager()
        self.max_shared_memory = max_shared_memory
        self.shared_input_array_config = self.manager.dict()
        self.shared_output_array_config = self.manager.dict()
        self.shared_input_arrays = {}
        self.shared_output_arrays = {}
        self.shared_input_memory = mp.RawArray(
            ctypes.c_float,
            self.max_shared_memory)
        self.shared_output_memory = mp.RawArray(
            ctypes.c_float,
            self.max_shared_memory)

        self.send_lock = mp.Lock()
        self.receive_lock = mp.Lock()
        self.worker_sent_inputs = mp.Event()
        self.predict_received_inputs = mp.Event()
        self.predict_sent_outputs = mp.Event()

        self.predict_process = mp.Process(target=self.__predict)
        self.predict_process_crashed = mp.Value('i', False)
        self.predict_process.start()

    def predict(self, batch, request):

        if self.skip_empty:

            if self.shared_output_arrays:
                # we cannot skip if `shared_output_arrays` is not initialized
                # prediction needs to run once first to init config

                can_skip = True
                for array_key in self.inputs.values():
                    if batch[array_key].data.sum() != 0:
                        can_skip = False
                        break

                if can_skip:

                    logger.info(
                        "Skipping batch %i (all inputs are 0)" % batch.id)

                    for name, array_key in self.outputs.items():

                        shape = self.shared_output_arrays[name].shape
                        dtype = self.shared_output_arrays[name].dtype

                        spec = self.spec[array_key].copy()
                        spec.roi = request[array_key].roi.copy()
                        batch.arrays[array_key] = Array(
                            np.zeros(shape, dtype=dtype),
                            spec)

                    return

        logger.debug("predicting in batch %i", batch.id)

        output_tensors = self.__collect_outputs(request)
        input_data = self.__collect_provided_inputs(batch)

        self.send_lock.acquire()

        self.__write_inputs_to_shared(input_data)
        self.worker_sent_inputs.set()

        self.receive_lock.acquire()

        self.predict_received_inputs.wait()
        self.__check_background_process([self.receive_lock, self.send_lock])
        self.predict_received_inputs.clear()

        self.send_lock.release()

        self.predict_sent_outputs.wait()
        self.predict_sent_outputs.clear()

        output_data = self.__read_outputs_from_shared(output_tensors)

        self.receive_lock.release()

        for array_key in output_tensors:
            spec = self.spec[array_key].copy()
            spec.roi = request[array_key].roi
            batch.arrays[array_key] = Array(
                output_data[array_key],
                spec)

        logger.debug("predicted in batch %i", batch.id)

    def _init_model(self):
        raise RuntimeError("Unimplemented")

    def _compute_prediction(self, input_data):
        raise RuntimeError("Unimplemented")

    def __predict(self):
        '''The background predict process.'''

        try:
            self._init_model()

            while True:

                # wait for inputs
                self.worker_sent_inputs.wait()
                self.worker_sent_inputs.clear()

                # read inputs
                input_data = self.__read_inputs_from_shared()
                self.predict_received_inputs.set()

                # compute outputs
                output_data = self._compute_prediction(input_data)

                # write outputs
                self.__write_outputs_to_shared(output_data)
                self.predict_sent_outputs.set()

        except Exception as e:

            self.predict_process_crashed.value = True

            # release locks and events
            self.worker_sent_inputs.clear()
            self.predict_received_inputs.set()
            self.predict_sent_outputs.set()
            raise e

    def teardown(self):

        self.predict_process.terminate()
        self.predict_process.join()

    def __check_background_process(self, locks=[]):

        if self.predict_process_crashed.value:
            # release all locks before raising exception
            for lock in locks:
                lock.release()
            raise RuntimeError("Background process died.")

    def __collect_outputs(self, request=None):
        '''Get a dict:

            array key: tensor name

        If request is not None, return only outputs that are in request.
        '''

        array_outputs = {}

        for tensor_name, array_key in self.outputs.items():
            if request is None or array_key in request:
                array_outputs[array_key] = tensor_name

        return array_outputs

    def __collect_provided_inputs(self, batch):
        '''Get a dict:

            tensor name: ndarray
        '''

        inputs = {}

        for input_name, input_key in self.inputs.items():
            if isinstance(input_key, ArrayKey):
                if input_key in batch.arrays:
                    inputs[input_name] = batch.arrays[input_key].data
                else:
                    logger.warn("batch does not contain %s, input %s will not "
                                "be set", input_key, input_name)
            elif isinstance(input_key, np.ndarray):
                inputs[input_name] = input_key
            else:
                raise Exception(
                    "Unknown network input key {}, can't be given to "
                    "network".format(input_key))

        return inputs

    def __create_shared_input_array_config(self, input_data):
        '''Store the shared array config in a shared dictionary. Should be run
        once by the first worker to submit a batch.'''

        begin = 0
        for name, _ in self.inputs.items():

            shape = input_data[name].shape
            size = reduce(mul, shape, 1)
            dtype = input_data[name].dtype

            self.shared_input_array_config[name] = (
                begin,
                size,
                shape,
                dtype)

            begin += size*np.dtype(dtype).itemsize
            assert begin <= self.max_shared_memory, (
                "The input arrays exceed the max_shared_memory")

    def __create_shared_output_array_config(self, output_data):
        '''To be called by predict process.'''

        begin = 0
        for name, _ in self.outputs.items():

            shape = output_data[name].shape
            size = reduce(mul, shape, 1)
            dtype = output_data[name].dtype

            self.shared_output_array_config[name] = (
                begin,
                size,
                shape,
                dtype)

            begin += size*np.dtype(dtype).itemsize
            assert begin <= self.max_shared_memory, (
                "The output arrays exceed the max_shared_memory")

    def __init_shared_input_arrays(self):
        '''Assign the shared memory to numpy arrays.'''

        for name, (begin, size, shape, dtype) in self.shared_input_array_config.items():

            self.shared_input_arrays[name] = np.frombuffer(
                self.shared_input_memory,
                dtype=dtype,
                offset=begin,
                count=size).reshape(shape)

    def __init_shared_output_arrays(self):
        '''Assign the shared memory to numpy arrays.'''

        for name, (begin, size, shape, dtype) in self.shared_output_array_config.items():

            self.shared_output_arrays[name] = np.frombuffer(
                self.shared_output_memory,
                dtype=dtype,
                offset=begin,
                count=size).reshape(shape)

    def __write_inputs_to_shared(self, input_data):

        if not self.shared_input_arrays:
            if not self.shared_input_array_config:
                self.__create_shared_input_array_config(input_data)
            self.__init_shared_input_arrays()

        for tensor_name, data in input_data.items():
            self.shared_input_arrays[tensor_name][:] = data

    def __read_inputs_from_shared(self):

        if not self.shared_input_arrays:
            assert self.shared_input_array_config
            self.__init_shared_input_arrays()

        return {
            tensor_name: self.shared_input_arrays[tensor_name].copy()
            for tensor_name in self.inputs.keys()
        }

    def __write_outputs_to_shared(self, output_data):

        if not self.shared_output_arrays:
            if not self.shared_output_array_config:
                self.__create_shared_output_array_config(output_data)
            self.__init_shared_output_arrays()

        for tensor_name, data in output_data.items():
            self.shared_output_arrays[tensor_name][:] = data

    def __read_outputs_from_shared(self, output_tensors):

        if not self.shared_output_arrays:
            assert self.shared_output_array_config
            self.__init_shared_output_arrays()

        return {
                array_key: self.shared_output_arrays[tensor_name].copy()
                for array_key, tensor_name in output_tensors.items()
        }
